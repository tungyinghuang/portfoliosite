<html>
  <head>
    <meta charset="utf-8">
    <link href="CSS/style.css" rel="stylesheet">
    <title>About|Tung-Ying Huang</title>
  </head>
  <body>

      <div class="topnav">
        <a class="active"href="index.html#works">Works</a>
        <a href="about.html">About</a>

      </div>

     <div class="post">
       <div class="title">
         <h2> <span>Scerf, the smart shopping cart.</span></h2>
         <img srcset="images/post/serf-02.png 1000w,
                      images/post/serf-02.png 2000w"
              src="images/post/serf-02.png">
       </div>
         <div class="worksblock">
           <div class="worksnavs">
               <a href="#intro">Introduction</a>
               <a href="#meaning">Meaning</a>
               <a href="#role">My role</a>

           </div>

           <div id="intro" class="title">
             <h3>This is the introduction!</h3>
             <p>The method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination. Applied to 3-branch residual networks, shake-shake regularization improves on the best single shot published results on CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%. Experiments on architectures without skip connections or Batch Normalization show encouraging results and open the door to a large set of</p>
           </div>
           <div id="meaning"class="title">
             <h3>This is the meaning!</h3>
             <p>The method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination. Applied to 3-branch residual networks, shake-shake regularization improves on the best single shot published results on CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%. Experiments on architectures without skip connections or Batch Normalization show encouraging results and open the door to a large set of</p>
           </div>
           <div id="role"class="title">
             <h3>This is my role!</h3>
             <p>The method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination. Applied to 3-branch residual networks, shake-shake regularization improves on the best single shot published results on CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%. Experiments on architectures without skip connections or Batch Normalization show encouraging results and open the door to a large set of</p>
           </div>


       </div>


  </body>

</html>
